

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/blog/img/favicon.png">
  <link rel="icon" href="/blog/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="">
  <meta name="keywords" content="">
  
  <title>Elasticsearch集群 - 萤火的博客</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.5.3/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/blog/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.1.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/blog/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"zhaoguocheng.gitee.io","root":"/blog/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":200}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/blog/js/utils.js" ></script>
  <script  src="/blog/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/blog/">&nbsp;<strong>萤火的博客</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/">
                <i class="iconfont icon-home-fill"></i>
                主页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/blog/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Elasticsearch集群">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-10-12 10:44" pubdate>
        2022年10月12日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      13.5k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      171
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Elasticsearch集群</h1>
            
            <div class="markdown-body">
              <h3 id="master-node和data-node两种角色"><a href="#master-node和data-node两种角色" class="headerlink" title="master node和data node两种角色"></a>master node和data node两种角色</h3><p>es是一种peer to peer，也就是p2p点对点的分布式系统架构，不是hadoop生态普遍采用的那种master-slave主从架构的分布式系统。集群中的每个node是直接跟其他节点进行通信的，几乎所有的API操作，比如index，delete，search等，都不是client跟master通信，而是client跟任何一个node进行通信，那个node再将请求转发给对应的node来进行执行。</p>
<p>es集群中角色分为master node和data node。正常情况下就只有一个master node。master node的责任就是负责维护整个集群的状态信息，也就是一些集群元数据信息。同时在node加入集群或者从集群中下线时，重新分配shard。包括每次cluster state如果有改变的化，master都会负责将集群状态同步给所有的node，集群中所有的node都有一份完整的cluster state。master之外的node负责数据的存储和读写的，比如写入索引，搜索数据等操作。</p>
<p>在进行es集群规划时，可选节点类型为：master eligible node和data node，其中master eligible node为候选master节点，master node将从从这些master eligible node中选举产生，master node故障时，只有master eligible node具有接替他的资格。一般建议master eligible node给3个即可，配置方式为：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">node.master:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">node.data:</span> <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure>

<p>默认情况下两项配置都为true，使节点同时具有master eligible node跟data note的功能。一般来说10个以内的节点，那就所有节点都可以作为master eligible node以及data node即可，不需要额外配置，超过10个node的集群再单独拆分master和data node。</p>
<h3 id="zen-discovery集群发现机制"><a href="#zen-discovery集群发现机制" class="headerlink" title="zen discovery集群发现机制"></a>zen discovery集群发现机制</h3><p>默认情况下，es进程会绑定在本机的回环地址上，也就是127.0.0.1，然后扫描9300~9305端口号，尝试跟那些端口上启动的其他es进程进行通信，组成一个集群。对于生产环境而言，需要将每台es进程绑定在一个非回环的ip地址上，才能跟其他节点进行通信，同时需要使用集群发现机制来跟其他节点上的es node进行通信。</p>
<p>一般会配置：network.host: 192.168.1.10。一旦配置了network.host，那么es就会从开发模式迁移到生产模式，同时会启用一系列的bootstrap check。</p>
<p>如果在本机上启动多个es进程，es就会自己组成一个集群。在生产环境上，就涉及到es的discovery机制，通过集群中各个节点的互相发现，组成一个集群机制。同时discovery机制也负责master选举。</p>
<p>es中默认的discovery机制，就是zen discovery机制。zen discovery机制提供了unicast discovery集群发现机制，集群发现时的节点间通信是依赖的transport module，也就是es底层的网络通信模块和协议。</p>
<p>es默认配置为使用unicast集群发现机制，可以让经过特殊配置的节点组成一个集群，而不是随便哪个节点都可以组成一个集群。但是默认配置下，unicast是本机，也就是localhost，因此只能在一台机器上启动多个node来组成一个集群。同时es还提供multicast plugin作为一个发现机制，但是已经不建议在生产环境中使用了。multicast机制比较简单，就是所有的node可以在接收到一条multicast ping之后就立即自动加入集群，但是multicast机制有很多的问题，而且很脆弱，比如网络有轻微的调整，就可能导致节点无法发现对方。因此现在建议在生产环境中用unicast机制，提供一个es种子node作为中转路由节点就可以了。</p>
<p> 如果要让多个node组成一个es集群，首先第一个要设置的参数，就是cluster.name，多个node的cluster.name如果一样，才满足组成一个集群的基本条件。cluster.name默认为elasticsearch，在生产环境中，一定要修改这个值，否则可能会导致未知的node无端加入集群，造成集群运行异常。</p>
<h4 id="ping"><a href="#ping" class="headerlink" title="ping"></a>ping</h4><p>ping是一个node用discovery机制来发现其他node的一个过程</p>
<h4 id="unicast"><a href="#unicast" class="headerlink" title="unicast"></a>unicast</h4><p>unicast discovery集群发现机制是要求配置一个主机列表，用来作为gossip（流言式）通信协议的路由器。这些机器如果通过hostname来指定，那么在ping的时候会被解析为ip地址。unicast discovery机制最重要的两个配置如下所示：</p>
<ul>
<li>主机列表：<code>discovery.zen.ping.unicast.hosts: [&quot;host1&quot;,&quot;host2&quot;,&quot;host3&quot;]</code></li>
<li>超时时间：<code>discovery.zen.ping_timeout: 60s</code></li>
</ul>
<p>基于gossip流言式通信协议的unicast集群发现机制，简单来说就是所有节点将信息发送到中间的公共节点，同时从公共节点获取其他节点的信息，通过交换信息感知到其他节点的存在，并且进行通信，最后组成一个集群。</p>
<p>当一个node与unicast node list中的一个成员通信之后，就会接收到一份完整的集群状态，这里会列出集群中所有的node。接着那个node再通过cluster state跟master通信，并且加入集群中。这就意味着unicast list node是不需要列出集群中的所有节点的，只要提供少数几个node，比如3个，让新的node可以连接上即可。如果我们给集群中分配了几个节点作为专门的master节点，那么只要列出我们那三个专门的master节点即可。</p>
<h4 id="master选举"><a href="#master选举" class="headerlink" title="master选举"></a>master选举</h4><p>在ping发现过程中，es集群会自动完成master选举，在完成一个集群的master选举之后，每次一个新的node加入集群，都会发送一个join request到master node。</p>
<p>ping的超时时间可以通过discovery.zen.ping_timeout参数（默认是3s）设置，如果因为网络慢或者拥塞，导致master选举超时，那么可以增加这个参数，确保集群启动的稳定性。</p>
<p>join request的超时时间可以通过discovery.zen.join_timeout设置，如果一次join不上，默认会重试20次。</p>
<p>如果master node被停止了，或者自己宕机了，那么集群中的node会再次进行一次ping过程，并且选举出一个新的master。如果discovery.zen.master_election.ignore_non_master_pings设置为了true，那么会强制区分master候选节点，node.master设置为false的节点发送的ping请求会被忽略。</p>
<p>discovery.zen.minimum_master_nodes参数用于设置对于一个新选举的master，必须有多少个master候选node去连接那个新选举的master。而且还用于设置一个集群中必须拥有的master的候选node数量。如果这些要求没有被满足，那么当前的master node就会被停止，重新选举一个新的master。这个参数必须设置为我们的master候选node的quorum数量。一般避免说只有两个master候选node，因为2的quorum还是2。如果在那个情况下，任何一个master候选节点宕机了，集群就无法正常运作了。</p>
<p>配置项统计</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">cluster</span><span class="hljs-selector-class">.name</span><br><span class="hljs-selector-tag">node</span><span class="hljs-selector-class">.name</span><br><span class="hljs-selector-tag">network</span><span class="hljs-selector-class">.host</span><br><span class="hljs-selector-tag">discovery</span><span class="hljs-selector-class">.zen</span><span class="hljs-selector-class">.ping</span><span class="hljs-selector-class">.unicast</span><span class="hljs-selector-class">.hosts</span><br></code></pre></td></tr></table></figure>

<p>此时各个节点：</p>
<ol>
<li>通过network.host绑定到了非回环的ip地址，从而可以跟其他节点通信。</li>
<li>通过discovery.zen.ping.unicast.hosts配置了一批unicast中间路由的node</li>
<li>所有node都可以发送ping消息到路由node，再从路由node获取cluster state回来</li>
<li>接着所有node会选举出一个master</li>
<li>所有node都会跟master进行通信，然后加入master的集群</li>
<li>要求cluster.name必须一样，才能组成一个集群</li>
<li>node.name就标识出了每个node我们自己设置的一个名称</li>
</ol>
<h3 id="集群故障排查"><a href="#集群故障排查" class="headerlink" title="集群故障排查"></a>集群故障排查</h3><p>es有两种集群故障探查机制，第一种是通过master进行的，master会ping集群中所有的其他node，确保它们是否是存活着的。第二种，每个node都会去ping master node来确保master node是存活的，否则就会发起一个选举过程。</p>
<p>有下面三个参数用来配置集群故障的探查过程：</p>
<ul>
<li>ping_interval：每隔多长时间会ping一次node，默认是1s</li>
<li>ping_timeout：每次ping的timeout等待时长是多长时间，默认是30s</li>
<li>ping_retries：如果一个node被ping多少次都失败了，就会认为node故障，默认是3次</li>
</ul>
<h3 id="集群状态更新"><a href="#集群状态更新" class="headerlink" title="集群状态更新"></a>集群状态更新</h3><p>master node是集群中唯一一个可以对cluster state进行更新的node。master node每次会处理一个集群状态的更新事件，应用这次状态更新，然后将更新后的状态发布到集群中所有的node上去。每个node都会接收publish message，ack这个message，但是不会应用这个更新。如果master没有在discovery.zen.commit_timeout指定的时间内（默认是30s），从至少discovery.zen.minimum_master_nodes个节点获取ack响应，那么这次cluster state change事件就会被reject，不会应用。</p>
<p>如果在指定时间内，指定数量的node都返回了ack消息，那么cluster state就会被commit，然后将commit message会被发送给所有的node。所有的node接收到那个commit message之后，接着才会将之前接收到的集群状态应用到自己本地的状态副本中去。接着master会等待所有节点再次响应是否更新自己本地副本状态成功，在一个等待超时时长内，如果接收到了响应，那么就会继续处理内存queue中保存的下一个更新状态。discovery.zen.publish_timeout默认是30s，这个超时等待时长是从plublish cluster state开始计算的。</p>
<h3 id="不因为master宕机阻塞集群操作"><a href="#不因为master宕机阻塞集群操作" class="headerlink" title="不因为master宕机阻塞集群操作"></a>不因为master宕机阻塞集群操作</h3><p>如果master节点宕机，discovery.zen.no_master_block可以控制，什么样的操作应该被拒绝。</p>
<ul>
<li>all：一旦master宕机，那么所有的操作都会被拒绝。</li>
<li>write：默认的选项，所有的写操作都会被拒绝，但是读操作是被允许的。</li>
</ul>
<p>集群正常运转的前提是，master节点正常运行，且discovery.zen.minimum_master_nodes指定数量的master候选node，都在运行。</p>
<h3 id="脑裂问题"><a href="#脑裂问题" class="headerlink" title="脑裂问题"></a>脑裂问题</h3><p>如果集群中的某几台机器无法与原集群通信，但是原集群仍正常，此时这几台机器也会进行master选举，这样集群中就会出现两个master节点。</p>
<p>通过discovery.zen.minimum_master_nodes可以有效的预防脑裂问题，这个参数的作用，就是告诉es直到有足够的master候选节点时，才可以选举出一个master，否则就不要选举出一个master。而且，master必须与足够数量的候选节点保持通信，否则就会结束当前master身份，进行重新选举。这个参数必须被设置为集群中master候选节点的quorum数量，也就是大多数。quorum数量的计算公式：master候选节点数量 / 2 + 1。</p>
<p>但是如果只有两个候选节点的话，quorum的数量就是2，此时master挂掉之后，候选节点数量是没法满足需求的，选举不出master集群就彻底挂了。如果设置为1，就没法预防脑裂的发生。所以，生产环境中的集群最少要有三个节点。</p>
<p>因为es集群是可以动态增加和下线节点的，所以可能随时会改变quorum。所以这个参数也是可以通过api随时修改的，特别是在节点上线和下线的时候，都需要作出对应的修改。而且一旦修改过后，这个配置就会持久化保存下来。</p>
<figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ada">PUT /_cluster/settings<br>&#123;<br>    <span class="hljs-string">&quot;persistent&quot;</span> : &#123;<br>        <span class="hljs-string">&quot;discovery.zen.minimum_master_nodes&quot;</span> : 2<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="shard-recovery配置"><a href="#shard-recovery配置" class="headerlink" title="shard recovery配置"></a>shard recovery配置</h3><p>集群重启时的无意义shard重分配问题</p>
<p>如果es集群中有10个node，当集群进行重启时，可能会出现部分节点先启动了，其余节点没启动的情况。这时先启动的节点会通过gossip协议互相通信，选举出一个master，然后组成集群。这时集群中的shard是没有均匀分布的，就会重新进行shard的reblance操作。但是当所有节点都启动加入集群后，就会发现他们持有的shard已经被重新分配，这时他们就会删除自己本地的shard，然后集群又会进行shard的rebalance操作，将最早启动的node上的shard重新分配。</p>
<p>在这个过程中，这些shard重新复制，移动，删除，再次移动的过程，会大量的耗费网络和磁盘资源。对于数据量庞大的集群来说，可能导致每次集群重启时，都有TB级别的数据无端移动，可能导致集群启动会耗费很长时间。但是如果所有的节点都可以等待整个集群中的所有节点都完全上线之后，所有的数据都有了以后，再决定是否要复制和移动shard，情况就会好很多。</p>
<p>这时可以设置一个参数，gateway.recover_after_nodes: 8。这个参数可以让es直到有足够的node都上线之后，再开始shard recovery的过程。所以这个参数是跟具体的集群相关的，要根据我们的集群中节点的数量来决定。此外，还应该设置一个集群中至少要有多少个node，等待那些node的时间：gateway.expected_nodes: 10，gateway.recover_after_time: 5m。经过上面的配置之后，es集群的行为会变成下面这样，等待至少8个节点在线，然后等待最多5分钟，或者10个节点都在线，开始shard recovery的过程。这样就可以避免少数node启动时，就立即开始shard recovery，消耗大量的网络和磁盘资源，甚至可以将shard recovery过程从数小时缩短为数分钟。</p>
<h3 id="集群状态检查"><a href="#集群状态检查" class="headerlink" title="集群状态检查"></a>集群状态检查</h3><h4 id="cat-API"><a href="#cat-API" class="headerlink" title="cat API"></a>cat API</h4><p>一般来说es集群监控都是自己开发的，根据es的一些api，自己写一个java web的应用，自己做前端界面，程序里不断的每隔几秒钟，调用一次后端的接口，获取到各种监控信息，然后用前端页面显示出来，开发开发一个可视化的es集群的监控的工作台。</p>
<p>在es老版本，有一个很好用的插件，叫做head，但是5.x之后都，es主推自己的x-pack，这个是要收费的。</p>
<p>GET /_cat/aliases?v</p>
<p>看到集群中有哪些索引别名</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">alias</span>  index filter routing.index routing.search<br><span class="hljs-attribute">alias1</span> test<span class="hljs-number">1</span> -      -            -<br><span class="hljs-attribute">alias2</span> test<span class="hljs-number">1</span> *      -            -<br><span class="hljs-attribute">alias3</span> test<span class="hljs-number">1</span> -      <span class="hljs-number">1</span>            <span class="hljs-number">1</span><br><span class="hljs-attribute">alias4</span> test<span class="hljs-number">1</span> -      <span class="hljs-number">2</span>            <span class="hljs-number">1</span>,<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure>

<p>GET /_cat/allocation?v</p>
<p>看到每个节点分配了几个shard，对磁盘的占用空间大小，使用率，等等</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">shards</span> disk.indices disk.used disk.avail disk.total disk.percent host      ip        node<br>     <span class="hljs-attribute">5</span>         <span class="hljs-number">260</span>b    <span class="hljs-number">47</span>.<span class="hljs-number">3</span>gb     <span class="hljs-number">43</span>.<span class="hljs-number">4</span>gb    <span class="hljs-number">100</span>.<span class="hljs-number">7</span>gb           <span class="hljs-number">46</span> <span class="hljs-number">127.0.0.1</span> <span class="hljs-number">127.0.0.1</span> CSUXak<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure>

<p>GET /_cat/count?v</p>
<p>看每个索引的document数量</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">epoch</span>      timestamp count<br><span class="hljs-attribute">1475868259</span> <span class="hljs-number">15</span>:<span class="hljs-number">24</span>:<span class="hljs-number">20</span>  <span class="hljs-number">120</span><br></code></pre></td></tr></table></figure>

<p>GET /_cat/fielddata?v</p>
<p>看每个节点的jvm heap内存中的fielddata内存占用情况（对分词的field进行聚合/排序要用jvm heap中的正排索引，fielddata）</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">id</span>                     host      ip        node    field   size<br><span class="hljs-attribute">Nqk</span>-<span class="hljs-number">6</span>inXQq-OxUfOUI<span class="hljs-number">8</span>jNQ <span class="hljs-number">127.0.0.1</span> <span class="hljs-number">127.0.0.1</span> Nqk-<span class="hljs-number">6</span>in body    <span class="hljs-number">544</span>b<br><span class="hljs-attribute">Nqk</span>-<span class="hljs-number">6</span>inXQq-OxUfOUI<span class="hljs-number">8</span>jNQ <span class="hljs-number">127.0.0.1</span> <span class="hljs-number">127.0.0.1</span> Nqk-<span class="hljs-number">6</span>in soul    <span class="hljs-number">480</span>b<br></code></pre></td></tr></table></figure>

<p>GET /_cat/health?v</p>
<p>比较全面的看一个es集群的整体健康状况，主要是看是green，yellow，red</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">epoch</span>      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent<br><span class="hljs-attribute">1475871424</span> <span class="hljs-number">16</span>:<span class="hljs-number">17</span>:<span class="hljs-number">04</span>  elasticsearch green           <span class="hljs-number">1</span>         <span class="hljs-number">1</span>      <span class="hljs-number">5</span>   <span class="hljs-number">5</span>    <span class="hljs-number">0</span>    <span class="hljs-number">0</span>        <span class="hljs-number">0</span>             <span class="hljs-number">0</span>                  -                <span class="hljs-number">100</span>.<span class="hljs-number">0</span>%<br></code></pre></td></tr></table></figure>

<ul>
<li>green：每个索引的primary shard和replica shard都是active状态的</li>
<li>yellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态</li>
<li>red：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了</li>
</ul>
<p>GET /_cat/indices?v</p>
<p>每个索引的具体的情况，比如有几个shard，多少个document，被删除的document有多少，占用了多少磁盘空间</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">health</span> status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size<br><span class="hljs-attribute">yellow</span> open   twitter  u<span class="hljs-number">8</span>FNjxh<span class="hljs-number">8</span>Rfy_awN<span class="hljs-number">11</span>oDKYQ   <span class="hljs-number">1</span>   <span class="hljs-number">1</span>       <span class="hljs-number">1200</span>            <span class="hljs-number">0</span>     <span class="hljs-number">88</span>.<span class="hljs-number">1</span>kb         <span class="hljs-number">88</span>.<span class="hljs-number">1</span>kb<br></code></pre></td></tr></table></figure>

<p>GET /_cat/master?v</p>
<p>看master node当前的具体的情况，哪个node是当前的master node</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">id</span>                     host      ip        node<br><span class="hljs-attribute">YzWoH_2BT</span>-<span class="hljs-number">6</span>UjVGDyPdqYg <span class="hljs-number">127.0.0.1</span> <span class="hljs-number">127.0.0.1</span> YzWoH_<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure>

<p>GET /_cat/nodes?v</p>
<p>看每个node的具体的情况，就比如jvm heap内存使用率，内存使用率，cpu load，是什么角色</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">ip</span>        heap.percent ram.percent cpu load_<span class="hljs-number">1</span>m load_<span class="hljs-number">5</span>m load_<span class="hljs-number">15</span>m node.role master name<br><span class="hljs-attribute">127</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span>           <span class="hljs-number">65</span>          <span class="hljs-number">99</span>  <span class="hljs-number">42</span>    <span class="hljs-number">3</span>.<span class="hljs-number">07</span>                  mdi       *      mJw<span class="hljs-number">06</span>l<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure>

<p>GET /_cat/pending_tasks?v</p>
<p>看当前pending没执行完的task的具体情况，执行的是什么操作</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs sql">insertOrder timeInQueue priority source<br>       1685       855ms HIGH     <span class="hljs-keyword">update</span>-<span class="hljs-keyword">mapping</span> [foo][t]<br>       <span class="hljs-number">1686</span>       <span class="hljs-number">843</span>ms <span class="hljs-keyword">HIGH</span>     <span class="hljs-keyword">update</span>-<span class="hljs-keyword">mapping</span> [foo][t]<br>       <span class="hljs-number">1693</span>       <span class="hljs-number">753</span>ms <span class="hljs-keyword">HIGH</span>     <span class="hljs-keyword">refresh</span>-<span class="hljs-keyword">mapping</span> [foo][[t]]<br>       <span class="hljs-number">1688</span>       <span class="hljs-number">816</span>ms <span class="hljs-keyword">HIGH</span>     <span class="hljs-keyword">update</span>-<span class="hljs-keyword">mapping</span> [foo][t]<br>       <span class="hljs-number">1689</span>       <span class="hljs-number">802</span>ms <span class="hljs-keyword">HIGH</span>     <span class="hljs-keyword">update</span>-<span class="hljs-keyword">mapping</span> [foo][t]<br>       <span class="hljs-number">1690</span>       <span class="hljs-number">787</span>ms <span class="hljs-keyword">HIGH</span>     <span class="hljs-keyword">update</span>-<span class="hljs-keyword">mapping</span> [foo][t]<br>       <span class="hljs-number">1691</span>       <span class="hljs-number">773</span>ms <span class="hljs-keyword">HIGH</span>     <span class="hljs-keyword">update</span>-<span class="hljs-keyword">mapping</span> [foo][t]<br></code></pre></td></tr></table></figure>

<p>GET /_cat/plugins?v&amp;s=component&amp;h=name,component,version,description</p>
<p>看当前集群安装了哪些插件</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">name</span>    component               version   description<br><span class="hljs-attribute">U7321H6</span> analysis-icu            <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The ICU Analysis plugin integrates Lucene ICU module into elasticsearch, adding ICU relates analysis components.<br><span class="hljs-attribute">U7321H6</span> analysis-kuromoji       <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The Japanese (kuromoji) Analysis plugin integrates Lucene kuromoji analysis module into elasticsearch.<br><span class="hljs-attribute">U7321H6</span> analysis-phonetic       <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The Phonetic Analysis plugin integrates phonetic token filter analysis with elasticsearch.<br><span class="hljs-attribute">U7321H6</span> analysis-smartcn        <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> Smart Chinese Analysis plugin integrates Lucene Smart Chinese analysis module into elasticsearch.<br><span class="hljs-attribute">U7321H6</span> analysis-stempel        <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The Stempel (Polish) Analysis plugin integrates Lucene stempel (polish) analysis module into elasticsearch.<br><span class="hljs-attribute">U7321H6</span> analysis-ukrainian        <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The Ukrainian Analysis plugin integrates the Lucene UkrainianMorfologikAnalyzer into elasticsearch.<br><span class="hljs-attribute">U7321H6</span> discovery-azure-classic <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The Azure Classic Discovery plugin allows to use Azure Classic API for the unicast discovery mechanism<br><span class="hljs-attribute">U7321H6</span> discovery-ec<span class="hljs-number">2</span>           <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The EC<span class="hljs-number">2</span> discovery plugin allows to use AWS API for the unicast discovery mechanism.<br><span class="hljs-attribute">U7321H6</span> discovery-file          <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> Discovery file plugin enables unicast discovery from hosts stored in a file.<br><span class="hljs-attribute">U7321H6</span> discovery-gce           <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The Google Compute Engine (GCE) Discovery plugin allows to use GCE API for the unicast discovery mechanism.<br><span class="hljs-attribute">U7321H6</span> ingest-attachment       <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> Ingest processor that uses Apache Tika to extract contents<br><span class="hljs-attribute">U7321H6</span> ingest-geoip            <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> Ingest processor that uses looksup geo data based <span class="hljs-literal">on</span> ip adresses using the Maxmind geo database<br><span class="hljs-attribute">U7321H6</span> ingest-user-agent       <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> Ingest processor that extracts information from a user agent<br><span class="hljs-attribute">U7321H6</span> jvm-example             <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> Demonstrates <span class="hljs-literal">all</span> the pluggable Java entry points in Elasticsearch<br><span class="hljs-attribute">U7321H6</span> lang-javascript         <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The JavaScript language plugin allows to have javascript as the language of scripts to execute.<br><span class="hljs-attribute">U7321H6</span> lang-python             <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The Python language plugin allows to have python as the language of scripts to execute.<br><span class="hljs-attribute">U7321H6</span> mapper-attachments      <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The mapper attachments plugin adds the attachment type to Elasticsearch using Apache Tika.<br><span class="hljs-attribute">U7321H6</span> mapper-murmur<span class="hljs-number">3</span>          <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The Mapper Murmur<span class="hljs-number">3</span> plugin allows to compute hashes of a field&#x27;s values at index-time and to store them in the index.<br><span class="hljs-attribute">U7321H6</span> mapper-size             <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The Mapper Size plugin allows document to record their uncompressed size at index time.<br><span class="hljs-attribute">U7321H6</span> store-smb               <span class="hljs-number">5</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span> The Store SMB plugin adds support for SMB stores.<br></code></pre></td></tr></table></figure>

<p>GET _cat/recovery?v</p>
<p>看shard recovery恢复的一个过程的具体情况</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">index</span>   shard time type  stage source_host source_node target_host target_node repository snapshot files files_recovered files_percent files_total bytes bytes_recovered bytes_percent bytes_total translog_ops translog_ops_recovered translog_ops_percent<br><br><span class="hljs-attribute">twitter</span> <span class="hljs-number">0</span>     <span class="hljs-number">13</span>ms store done  n/a         n/a         node<span class="hljs-number">0</span>       node-<span class="hljs-number">0</span>      n/a        n/a      <span class="hljs-number">0</span>     <span class="hljs-number">0</span>               <span class="hljs-number">100</span>%          <span class="hljs-number">13</span>          <span class="hljs-number">0</span>     <span class="hljs-number">0</span>               <span class="hljs-number">100</span>%          <span class="hljs-number">9928</span>        <span class="hljs-number">0</span>            <span class="hljs-number">0</span>                      <span class="hljs-number">100</span>.<span class="hljs-number">0</span>%<br></code></pre></td></tr></table></figure>

<p>GET /_cat/repositories?v</p>
<p>查看用于snapshotting的repository有哪些</p>
<figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs ada">id    <span class="hljs-keyword">type</span><br>repo1   fs<br>repo2   s3<br></code></pre></td></tr></table></figure>

<p>GET /_cat/thread_pool?v</p>
<p>看每个线程池的具体的情况</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Z6MkIvC</span> bulk                <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br><span class="hljs-attribute">Z6MkIvC</span> fetch_shard_started <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br><span class="hljs-attribute">Z6MkIvC</span> fetch_shard_store   <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>

<h4 id="cluster-API"><a href="#cluster-API" class="headerlink" title="cluster API"></a>cluster API</h4><p>GET _cluster/health</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs json">&#123;<br>  <span class="hljs-attr">&quot;cluster_name&quot;</span> : <span class="hljs-string">&quot;testcluster&quot;</span>,<br>  <span class="hljs-attr">&quot;status&quot;</span> : <span class="hljs-string">&quot;yellow&quot;</span>,<br>  <span class="hljs-attr">&quot;timed_out&quot;</span> : <span class="hljs-literal">false</span>,<br>  <span class="hljs-attr">&quot;number_of_nodes&quot;</span> : <span class="hljs-number">1</span>,<br>  <span class="hljs-attr">&quot;number_of_data_nodes&quot;</span> : <span class="hljs-number">1</span>,<br>  <span class="hljs-attr">&quot;active_primary_shards&quot;</span> : <span class="hljs-number">5</span>,<br>  <span class="hljs-attr">&quot;active_shards&quot;</span> : <span class="hljs-number">5</span>,<br>  <span class="hljs-attr">&quot;relocating_shards&quot;</span> : <span class="hljs-number">0</span>,<br>  <span class="hljs-attr">&quot;initializing_shards&quot;</span> : <span class="hljs-number">0</span>,<br>  <span class="hljs-attr">&quot;unassigned_shards&quot;</span> : <span class="hljs-number">5</span>,<br>  <span class="hljs-attr">&quot;delayed_unassigned_shards&quot;</span>: <span class="hljs-number">0</span>,<br>  <span class="hljs-attr">&quot;number_of_pending_tasks&quot;</span> : <span class="hljs-number">0</span>,<br>  <span class="hljs-attr">&quot;number_of_in_flight_fetch&quot;</span>: <span class="hljs-number">0</span>,<br>  <span class="hljs-attr">&quot;task_max_waiting_in_queue_millis&quot;</span>: <span class="hljs-number">0</span>,<br>  <span class="hljs-attr">&quot;active_shards_percent_as_number&quot;</span>: <span class="hljs-number">50.0</span><br>&#125;<br></code></pre></td></tr></table></figure>

<p>GET _cluster/stats?human&amp;pretty</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><code class="hljs json">&#123;<br>   <span class="hljs-attr">&quot;timestamp&quot;</span>: <span class="hljs-number">1459427693515</span>,<br>   <span class="hljs-attr">&quot;cluster_name&quot;</span>: <span class="hljs-string">&quot;elasticsearch&quot;</span>,<br>   <span class="hljs-attr">&quot;status&quot;</span>: <span class="hljs-string">&quot;green&quot;</span>,<br>   <span class="hljs-attr">&quot;indices&quot;</span>: &#123;<br>      <span class="hljs-attr">&quot;count&quot;</span>: <span class="hljs-number">2</span>,<br>      <span class="hljs-attr">&quot;shards&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;total&quot;</span>: <span class="hljs-number">10</span>,<br>         <span class="hljs-attr">&quot;primaries&quot;</span>: <span class="hljs-number">10</span>,<br>         <span class="hljs-attr">&quot;replication&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;index&quot;</span>: &#123;<br>            <span class="hljs-attr">&quot;shards&quot;</span>: &#123;<br>               <span class="hljs-attr">&quot;min&quot;</span>: <span class="hljs-number">5</span>,<br>               <span class="hljs-attr">&quot;max&quot;</span>: <span class="hljs-number">5</span>,<br>               <span class="hljs-attr">&quot;avg&quot;</span>: <span class="hljs-number">5</span><br>            &#125;,<br>            <span class="hljs-attr">&quot;primaries&quot;</span>: &#123;<br>               <span class="hljs-attr">&quot;min&quot;</span>: <span class="hljs-number">5</span>,<br>               <span class="hljs-attr">&quot;max&quot;</span>: <span class="hljs-number">5</span>,<br>               <span class="hljs-attr">&quot;avg&quot;</span>: <span class="hljs-number">5</span><br>            &#125;,<br>            <span class="hljs-attr">&quot;replication&quot;</span>: &#123;<br>               <span class="hljs-attr">&quot;min&quot;</span>: <span class="hljs-number">0</span>,<br>               <span class="hljs-attr">&quot;max&quot;</span>: <span class="hljs-number">0</span>,<br>               <span class="hljs-attr">&quot;avg&quot;</span>: <span class="hljs-number">0</span><br>            &#125;<br>         &#125;<br>      &#125;,<br>      <span class="hljs-attr">&quot;docs&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;count&quot;</span>: <span class="hljs-number">10</span>,<br>         <span class="hljs-attr">&quot;deleted&quot;</span>: <span class="hljs-number">0</span><br>      &#125;,<br>      <span class="hljs-attr">&quot;store&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-string">&quot;16.2kb&quot;</span>,<br>         <span class="hljs-attr">&quot;size_in_bytes&quot;</span>: <span class="hljs-number">16684</span>,<br>         <span class="hljs-attr">&quot;throttle_time&quot;</span>: <span class="hljs-string">&quot;0s&quot;</span>,<br>         <span class="hljs-attr">&quot;throttle_time_in_millis&quot;</span>: <span class="hljs-number">0</span><br>      &#125;,<br>      <span class="hljs-attr">&quot;fielddata&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;memory_size&quot;</span>: <span class="hljs-string">&quot;0b&quot;</span>,<br>         <span class="hljs-attr">&quot;memory_size_in_bytes&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;evictions&quot;</span>: <span class="hljs-number">0</span><br>      &#125;,<br>      <span class="hljs-attr">&quot;query_cache&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;memory_size&quot;</span>: <span class="hljs-string">&quot;0b&quot;</span>,<br>         <span class="hljs-attr">&quot;memory_size_in_bytes&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;total_count&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;hit_count&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;miss_count&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;cache_size&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;cache_count&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;evictions&quot;</span>: <span class="hljs-number">0</span><br>      &#125;,<br>      <span class="hljs-attr">&quot;completion&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-string">&quot;0b&quot;</span>,<br>         <span class="hljs-attr">&quot;size_in_bytes&quot;</span>: <span class="hljs-number">0</span><br>      &#125;,<br>      <span class="hljs-attr">&quot;segments&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;count&quot;</span>: <span class="hljs-number">4</span>,<br>         <span class="hljs-attr">&quot;memory&quot;</span>: <span class="hljs-string">&quot;8.6kb&quot;</span>,<br>         <span class="hljs-attr">&quot;memory_in_bytes&quot;</span>: <span class="hljs-number">8898</span>,<br>         <span class="hljs-attr">&quot;terms_memory&quot;</span>: <span class="hljs-string">&quot;6.3kb&quot;</span>,<br>         <span class="hljs-attr">&quot;terms_memory_in_bytes&quot;</span>: <span class="hljs-number">6522</span>,<br>         <span class="hljs-attr">&quot;stored_fields_memory&quot;</span>: <span class="hljs-string">&quot;1.2kb&quot;</span>,<br>         <span class="hljs-attr">&quot;stored_fields_memory_in_bytes&quot;</span>: <span class="hljs-number">1248</span>,<br>         <span class="hljs-attr">&quot;term_vectors_memory&quot;</span>: <span class="hljs-string">&quot;0b&quot;</span>,<br>         <span class="hljs-attr">&quot;term_vectors_memory_in_bytes&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;norms_memory&quot;</span>: <span class="hljs-string">&quot;384b&quot;</span>,<br>         <span class="hljs-attr">&quot;norms_memory_in_bytes&quot;</span>: <span class="hljs-number">384</span>,<br>         <span class="hljs-attr">&quot;doc_values_memory&quot;</span>: <span class="hljs-string">&quot;744b&quot;</span>,<br>         <span class="hljs-attr">&quot;doc_values_memory_in_bytes&quot;</span>: <span class="hljs-number">744</span>,<br>         <span class="hljs-attr">&quot;index_writer_memory&quot;</span>: <span class="hljs-string">&quot;0b&quot;</span>,<br>         <span class="hljs-attr">&quot;index_writer_memory_in_bytes&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;version_map_memory&quot;</span>: <span class="hljs-string">&quot;0b&quot;</span>,<br>         <span class="hljs-attr">&quot;version_map_memory_in_bytes&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;fixed_bit_set&quot;</span>: <span class="hljs-string">&quot;0b&quot;</span>,<br>         <span class="hljs-attr">&quot;fixed_bit_set_memory_in_bytes&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;file_sizes&quot;</span>: &#123;&#125;<br>      &#125;,<br>      <span class="hljs-attr">&quot;percolator&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;num_queries&quot;</span>: <span class="hljs-number">0</span><br>      &#125;<br>   &#125;,<br>   <span class="hljs-attr">&quot;nodes&quot;</span>: &#123;<br>      <span class="hljs-attr">&quot;count&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;total&quot;</span>: <span class="hljs-number">1</span>,<br>         <span class="hljs-attr">&quot;data&quot;</span>: <span class="hljs-number">1</span>,<br>         <span class="hljs-attr">&quot;coordinating_only&quot;</span>: <span class="hljs-number">0</span>,<br>         <span class="hljs-attr">&quot;master&quot;</span>: <span class="hljs-number">1</span>,<br>         <span class="hljs-attr">&quot;ingest&quot;</span>: <span class="hljs-number">1</span><br>      &#125;,<br>      <span class="hljs-attr">&quot;versions&quot;</span>: [<br>         <span class="hljs-string">&quot;5.5.1&quot;</span><br>      ],<br>      <span class="hljs-attr">&quot;os&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;available_processors&quot;</span>: <span class="hljs-number">8</span>,<br>         <span class="hljs-attr">&quot;allocated_processors&quot;</span>: <span class="hljs-number">8</span>,<br>         <span class="hljs-attr">&quot;names&quot;</span>: [<br>            &#123;<br>               <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;Mac OS X&quot;</span>,<br>               <span class="hljs-attr">&quot;count&quot;</span>: <span class="hljs-number">1</span><br>            &#125;<br>         ],<br>         <span class="hljs-attr">&quot;mem&quot;</span> : &#123;<br>            <span class="hljs-attr">&quot;total&quot;</span> : <span class="hljs-string">&quot;16gb&quot;</span>,<br>            <span class="hljs-attr">&quot;total_in_bytes&quot;</span> : <span class="hljs-number">17179869184</span>,<br>            <span class="hljs-attr">&quot;free&quot;</span> : <span class="hljs-string">&quot;78.1mb&quot;</span>,<br>            <span class="hljs-attr">&quot;free_in_bytes&quot;</span> : <span class="hljs-number">81960960</span>,<br>            <span class="hljs-attr">&quot;used&quot;</span> : <span class="hljs-string">&quot;15.9gb&quot;</span>,<br>            <span class="hljs-attr">&quot;used_in_bytes&quot;</span> : <span class="hljs-number">17097908224</span>,<br>            <span class="hljs-attr">&quot;free_percent&quot;</span> : <span class="hljs-number">0</span>,<br>            <span class="hljs-attr">&quot;used_percent&quot;</span> : <span class="hljs-number">100</span><br>         &#125;<br>      &#125;,<br>      <span class="hljs-attr">&quot;process&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;cpu&quot;</span>: &#123;<br>            <span class="hljs-attr">&quot;percent&quot;</span>: <span class="hljs-number">9</span><br>         &#125;,<br>         <span class="hljs-attr">&quot;open_file_descriptors&quot;</span>: &#123;<br>            <span class="hljs-attr">&quot;min&quot;</span>: <span class="hljs-number">268</span>,<br>            <span class="hljs-attr">&quot;max&quot;</span>: <span class="hljs-number">268</span>,<br>            <span class="hljs-attr">&quot;avg&quot;</span>: <span class="hljs-number">268</span><br>         &#125;<br>      &#125;,<br>      <span class="hljs-attr">&quot;jvm&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;max_uptime&quot;</span>: <span class="hljs-string">&quot;13.7s&quot;</span>,<br>         <span class="hljs-attr">&quot;max_uptime_in_millis&quot;</span>: <span class="hljs-number">13737</span>,<br>         <span class="hljs-attr">&quot;versions&quot;</span>: [<br>            &#123;<br>               <span class="hljs-attr">&quot;version&quot;</span>: <span class="hljs-string">&quot;1.8.0_74&quot;</span>,<br>               <span class="hljs-attr">&quot;vm_name&quot;</span>: <span class="hljs-string">&quot;Java HotSpot(TM) 64-Bit Server VM&quot;</span>,<br>               <span class="hljs-attr">&quot;vm_version&quot;</span>: <span class="hljs-string">&quot;25.74-b02&quot;</span>,<br>               <span class="hljs-attr">&quot;vm_vendor&quot;</span>: <span class="hljs-string">&quot;Oracle Corporation&quot;</span>,<br>               <span class="hljs-attr">&quot;count&quot;</span>: <span class="hljs-number">1</span><br>            &#125;<br>         ],<br>         <span class="hljs-attr">&quot;mem&quot;</span>: &#123;<br>            <span class="hljs-attr">&quot;heap_used&quot;</span>: <span class="hljs-string">&quot;57.5mb&quot;</span>,<br>            <span class="hljs-attr">&quot;heap_used_in_bytes&quot;</span>: <span class="hljs-number">60312664</span>,<br>            <span class="hljs-attr">&quot;heap_max&quot;</span>: <span class="hljs-string">&quot;989.8mb&quot;</span>,<br>            <span class="hljs-attr">&quot;heap_max_in_bytes&quot;</span>: <span class="hljs-number">1037959168</span><br>         &#125;,<br>         <span class="hljs-attr">&quot;threads&quot;</span>: <span class="hljs-number">90</span><br>      &#125;,<br>      <span class="hljs-attr">&quot;fs&quot;</span>: &#123;<br>         <span class="hljs-attr">&quot;total&quot;</span>: <span class="hljs-string">&quot;200.6gb&quot;</span>,<br>         <span class="hljs-attr">&quot;total_in_bytes&quot;</span>: <span class="hljs-number">215429193728</span>,<br>         <span class="hljs-attr">&quot;free&quot;</span>: <span class="hljs-string">&quot;32.6gb&quot;</span>,<br>         <span class="hljs-attr">&quot;free_in_bytes&quot;</span>: <span class="hljs-number">35064553472</span>,<br>         <span class="hljs-attr">&quot;available&quot;</span>: <span class="hljs-string">&quot;32.4gb&quot;</span>,<br>         <span class="hljs-attr">&quot;available_in_bytes&quot;</span>: <span class="hljs-number">34802409472</span><br>      &#125;,<br>      <span class="hljs-attr">&quot;plugins&quot;</span>: [<br>        &#123;<br>          <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;analysis-icu&quot;</span>,<br>          <span class="hljs-attr">&quot;version&quot;</span>: <span class="hljs-string">&quot;5.5.1&quot;</span>,<br>          <span class="hljs-attr">&quot;description&quot;</span>: <span class="hljs-string">&quot;The ICU Analysis plugin integrates Lucene ICU module into elasticsearch, adding ICU relates analysis components.&quot;</span>,<br>          <span class="hljs-attr">&quot;classname&quot;</span>: <span class="hljs-string">&quot;org.elasticsearch.plugin.analysis.icu.AnalysisICUPlugin&quot;</span>,<br>          <span class="hljs-attr">&quot;has_native_controller&quot;</span>: <span class="hljs-literal">false</span><br>        &#125;,<br>        &#123;<br>          <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;ingest-geoip&quot;</span>,<br>          <span class="hljs-attr">&quot;version&quot;</span>: <span class="hljs-string">&quot;5.5.1&quot;</span>,<br>          <span class="hljs-attr">&quot;description&quot;</span>: <span class="hljs-string">&quot;Ingest processor that uses looksup geo data based on ip adresses using the Maxmind geo database&quot;</span>,<br>          <span class="hljs-attr">&quot;classname&quot;</span>: <span class="hljs-string">&quot;org.elasticsearch.ingest.geoip.IngestGeoIpPlugin&quot;</span>,<br>          <span class="hljs-attr">&quot;has_native_controller&quot;</span>: <span class="hljs-literal">false</span><br>        &#125;,<br>        &#123;<br>          <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;ingest-user-agent&quot;</span>,<br>          <span class="hljs-attr">&quot;version&quot;</span>: <span class="hljs-string">&quot;5.5.1&quot;</span>,<br>          <span class="hljs-attr">&quot;description&quot;</span>: <span class="hljs-string">&quot;Ingest processor that extracts information from a user agent&quot;</span>,<br>          <span class="hljs-attr">&quot;classname&quot;</span>: <span class="hljs-string">&quot;org.elasticsearch.ingest.useragent.IngestUserAgentPlugin&quot;</span>,<br>          <span class="hljs-attr">&quot;has_native_controller&quot;</span>: <span class="hljs-literal">false</span><br>        &#125;<br>      ]<br>   &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>GET _cluster/pending_tasks</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs json">&#123;<br>   <span class="hljs-attr">&quot;tasks&quot;</span>: [<br>      &#123;<br>         <span class="hljs-attr">&quot;insert_order&quot;</span>: <span class="hljs-number">101</span>,<br>         <span class="hljs-attr">&quot;priority&quot;</span>: <span class="hljs-string">&quot;URGENT&quot;</span>,<br>         <span class="hljs-attr">&quot;source&quot;</span>: <span class="hljs-string">&quot;create-index [foo_9], cause [api]&quot;</span>,<br>         <span class="hljs-attr">&quot;time_in_queue_millis&quot;</span>: <span class="hljs-number">86</span>,<br>         <span class="hljs-attr">&quot;time_in_queue&quot;</span>: <span class="hljs-string">&quot;86ms&quot;</span><br>      &#125;,<br>      &#123;<br>         <span class="hljs-attr">&quot;insert_order&quot;</span>: <span class="hljs-number">46</span>,<br>         <span class="hljs-attr">&quot;priority&quot;</span>: <span class="hljs-string">&quot;HIGH&quot;</span>,<br>         <span class="hljs-attr">&quot;source&quot;</span>: <span class="hljs-string">&quot;shard-started ([foo_2][1], node[tMTocMvQQgGCkj7QDHl3OA], [P], s[INITIALIZING]), reason [after recovery from shard_store]&quot;</span>,<br>         <span class="hljs-attr">&quot;time_in_queue_millis&quot;</span>: <span class="hljs-number">842</span>,<br>         <span class="hljs-attr">&quot;time_in_queue&quot;</span>: <span class="hljs-string">&quot;842ms&quot;</span><br>      &#125;,<br>      &#123;<br>         <span class="hljs-attr">&quot;insert_order&quot;</span>: <span class="hljs-number">45</span>,<br>         <span class="hljs-attr">&quot;priority&quot;</span>: <span class="hljs-string">&quot;HIGH&quot;</span>,<br>         <span class="hljs-attr">&quot;source&quot;</span>: <span class="hljs-string">&quot;shard-started ([foo_2][0], node[tMTocMvQQgGCkj7QDHl3OA], [P], s[INITIALIZING]), reason [after recovery from shard_store]&quot;</span>,<br>         <span class="hljs-attr">&quot;time_in_queue_millis&quot;</span>: <span class="hljs-number">858</span>,<br>         <span class="hljs-attr">&quot;time_in_queue&quot;</span>: <span class="hljs-string">&quot;858ms&quot;</span><br>      &#125;<br>  ]<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="集群安装"><a href="#集群安装" class="headerlink" title="集群安装"></a>集群安装</h3><p>编辑ifcfg-ens33内容如下（注释部分）</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">TYPE</span>=Ethernet<br><span class="hljs-attr">PROXY_METHOD</span>=none<br><span class="hljs-attr">BROWSER_ONLY</span>=<span class="hljs-literal">no</span><br><span class="hljs-attr">BOOTPROTO</span>=static  <span class="hljs-comment">#静态</span><br><span class="hljs-attr">DEFROUTE</span>=<span class="hljs-literal">yes</span><br><span class="hljs-attr">IPV4_FAILURE_FATAL</span>=<span class="hljs-literal">no</span><br><span class="hljs-attr">IPV6INIT</span>=<span class="hljs-literal">yes</span><br><span class="hljs-attr">IPV6_AUTOCONF</span>=<span class="hljs-literal">yes</span><br><span class="hljs-attr">IPV6_DEFROUTE</span>=<span class="hljs-literal">yes</span><br><span class="hljs-attr">IPV6_FAILURE_FATAL</span>=<span class="hljs-literal">no</span><br><span class="hljs-attr">IPV6_ADDR_GEN_MODE</span>=stable-privacy<br><span class="hljs-attr">NAME</span>=ens33<br><span class="hljs-attr">UUID</span>=<span class="hljs-number">1</span>e204253-b555-<span class="hljs-number">46</span>a4-bf54-<span class="hljs-number">20</span>bfc5fed802<br><span class="hljs-attr">DEVICE</span>=ens33<br><span class="hljs-attr">ONBOOT</span>=<span class="hljs-literal">yes</span>  <span class="hljs-comment">#网卡随系统一起启动</span><br><span class="hljs-attr">IPADDR</span>=<span class="hljs-number">192.168</span>.<span class="hljs-number">206.128</span>  <span class="hljs-comment">#自己配置ip</span><br><span class="hljs-attr">NETMASK</span>=<span class="hljs-number">255.255</span>.<span class="hljs-number">255.0</span>   <span class="hljs-comment">#掩码 和虚拟网络那里设置的相同</span><br><span class="hljs-attr">GATEWAY</span>=<span class="hljs-number">192.168</span>.<span class="hljs-number">206.2</span>   <span class="hljs-comment">#网关 和虚拟网络那里设置的相同</span><br><span class="hljs-attr">DNS1</span>=<span class="hljs-number">114.114</span>.<span class="hljs-number">114.114</span>  <span class="hljs-comment">#配置DNS域名解析</span><br></code></pre></td></tr></table></figure>

<p>更新yum源</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 备份</span><br>mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup<br>wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo<br><span class="hljs-comment"># 生成缓存</span><br>yum makecache<br><span class="hljs-comment"># 更新系统</span><br>yum -y update<br></code></pre></td></tr></table></figure>

<p>安装jdk</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 将jdk-8u131-linux-x64.rpm上传到虚拟机中</span><br><span class="hljs-comment"># 安装JDK</span><br>rpm -ivh jdk-8u131-linux-x64.rpm<br><span class="hljs-comment"># 配置jdk相关的环境变量</span><br>vi .bashrc<br><span class="hljs-comment"># 添加</span><br><span class="hljs-built_in">export</span> JAVA_HOME=/usr/java/latest<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$JAVA_HOME</span>/bin<br><span class="hljs-comment"># 刷新</span><br><span class="hljs-built_in">source</span> .bashrc<br><span class="hljs-comment"># 测试jdk安装是否成功</span><br>java -version<br></code></pre></td></tr></table></figure>

<p>关闭防火墙</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">systemctl stop firewalld.service<br>systemctl <span class="hljs-built_in">disable</span> firewalld.service<br></code></pre></td></tr></table></figure>

<p>vi /etc/hosts</p>
<p>配置本机的hostname到ip地址的映射</p>
<figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">192.168.200.151</span> node1<br><span class="hljs-number">192.168.200.152</span> node2<br><span class="hljs-number">192.168.200.153</span> node3<br></code></pre></td></tr></table></figure>

<p>配置ssh密钥登陆</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 生成本机的公钥 过程中不断敲回车即可，ssh-keygen命令默认会将公钥放在/root/.ssh目录下</span><br>ssh-keygen -t rsa<br><span class="hljs-comment"># 将公钥复制为authorized_keys文件，此时使用ssh连接本机就不需要输入密码了</span><br><span class="hljs-built_in">cd</span> /root/.ssh<br>cp id_rsa.pub authorized_keys<br><span class="hljs-comment"># 配置三台机器互相之间的ssh免密码登录</span><br><span class="hljs-comment"># 使用ssh-copy-id -i hostname命令将本机的公钥拷贝到指定机器的authorized_keys文件中</span><br>ssh-copy-id -i node1<br></code></pre></td></tr></table></figure>

<p>创建用户</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 创建用户</span><br>adduser elasticsearch<br>passwd elasticsearch<br><span class="hljs-comment"># 授权</span><br>chown -R elasticsearch /usr/<span class="hljs-built_in">local</span>/elasticsearch<br>chown -R elasticsearch /var/<span class="hljs-built_in">log</span>/elasticsearch<br>chown -R elasticsearch /var/data/elasticsearch<br>chown -R elasticsearch /var/plugin/elasticsearch<br>chown -R elasticsearch /etc/elasticsearch<br>chown -R elasticsearch /usr/<span class="hljs-built_in">local</span>/tmp<br></code></pre></td></tr></table></figure>

<p>修改/etc/security/limits.conf中的用户为elasticsearch，加入memlock的soft unlimited，内容为：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">elasticsearch</span> hard memlock unlimited<br><span class="hljs-attribute">elasticsearch</span> soft memlock unlimited<br><span class="hljs-attribute">elasticsearch</span> hard nproc   <span class="hljs-number">2048</span><br><span class="hljs-attribute">elasticsearch</span> hard nofile  <span class="hljs-number">65536</span><br><span class="hljs-attribute">elasticsearch</span> hard as      unlimited<br></code></pre></td></tr></table></figure>

<p>禁止swap</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">swapoff -a</span><br></code></pre></td></tr></table></figure>

<p>修改/etc/fstab文件，然后将所有包含swap的行都注释掉</p>
<p>虚拟内存设置</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">sysctl -w vm.<span class="hljs-attribute">max_map_count</span>=262144<br></code></pre></td></tr></table></figure>

<p>修改es配置</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">node</span><span class="hljs-selector-class">.name</span>: <span class="hljs-selector-tag">node-3</span><br><span class="hljs-selector-tag">path</span><span class="hljs-selector-class">.data</span>: /<span class="hljs-selector-tag">var</span>/<span class="hljs-selector-tag">data</span>/<span class="hljs-selector-tag">elasticsearch</span><br><span class="hljs-selector-tag">path</span><span class="hljs-selector-class">.logs</span>: /<span class="hljs-selector-tag">var</span>/<span class="hljs-selector-tag">log</span>/<span class="hljs-selector-tag">elasticsearch</span><br><span class="hljs-selector-tag">discovery</span><span class="hljs-selector-class">.zen</span><span class="hljs-selector-class">.ping</span><span class="hljs-selector-class">.unicast</span><span class="hljs-selector-class">.hosts</span>: <span class="hljs-selector-attr">[<span class="hljs-string">&quot;node1&quot;</span>, <span class="hljs-string">&quot;node2&quot;</span>,<span class="hljs-string">&quot;node3&quot;</span>]</span><br><span class="hljs-selector-tag">network</span><span class="hljs-selector-class">.host</span>: <span class="hljs-selector-tag">node3</span><br><span class="hljs-selector-tag">http</span><span class="hljs-selector-class">.port</span>: 9200<br></code></pre></td></tr></table></figure>

<p>指定配置文件启动es</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">.<span class="hljs-regexp">/bin/</span>elasticsearch -d -Epath.conf=<span class="hljs-regexp">/etc/</span>elasticsearch<br></code></pre></td></tr></table></figure>

<p>查看集群情况</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">curl</span> -XGET node<span class="hljs-number">1</span>:<span class="hljs-number">9200</span>/_cat/nodes?v<br></code></pre></td></tr></table></figure>



<h3 id="主要模块解析"><a href="#主要模块解析" class="headerlink" title="主要模块解析"></a>主要模块解析</h3><h4 id="shard-allocation"><a href="#shard-allocation" class="headerlink" title="shard allocation"></a>shard allocation</h4><p>索引中primary shard、replica shard的分配、shard的rebalance以及shard在node间的移动都是由master node来完成。es提供了很多设置，来控制这个过程：</p>
<ol>
<li>cluster level shard allocation，可以在集群层面来控制shard allocation和rebalance的过程</li>
<li>disk-based shard allocation，es会在shard分配的时候，考虑可用的磁盘空间</li>
<li>shard allocation awareness，控制shard如何在不同的机架上进行分布</li>
<li>shard allocation filtering，可以控制有些node不参与allocation的过程，这样的话，这些node就可以被安全的下线</li>
</ol>
<h5 id="cluster-level-shard-allocation"><a href="#cluster-level-shard-allocation" class="headerlink" title="cluster level shard allocation"></a>cluster level shard allocation</h5><p>shard allocation，就是将shard分配给node的一个过程，这个过程可能会在集群启动初始化进行恢复的时候发生，也会发生在replica shard被分配的时候，集群进行rebalance的时候，或者是有新的node加入，有旧的node被下线的时候。</p>
<p><strong>shard allocation settings</strong></p>
<ul>
<li><p>cluster.routing.allocation.same_shard.host：默认值是false，如果设置为true，那么就不允许将一个primary shard和replica shard分配到同一个物理机上，也许这个物理机上启动了多个es实例。</p>
</li>
<li><p>cluster.routing.allocation.enable</p>
<ul>
<li>all，默认，对所有类型的shard都允许分配</li>
<li>primaries，仅仅允许primary shard被分配</li>
<li>new_primaries，仅仅对新建索引的primary shard允许分配</li>
<li>none，不允许任何shard被分配</li>
</ul>
</li>
<li><p>cluster.routing.allocation.node_concurrent_incoming_recoveries</p>
<p>在一个node上允许同时恢复多少个shard，这里的shard recovery过程，指的就是要将某个shard分配给这个node。这个设置的默认值是2。</p>
<p>但是这个配置对node重启时本地primary shard的恢复没有影响，重启node的时候，如果本地有一个未被分配的primary shard，还是会立即恢复这个primary shard。</p>
</li>
<li><p>cluster.routing.allocation.node_concurrent_recoveries：同时设置上面两个值</p>
</li>
<li><p>cluster.routing.allocation.node_initial_primaries_recoveries：如果replica shard recovery通过网络传输来分配，那么一个未被分配的primary shard会在node重启之后使用本地磁盘上的数据，这个过程因为是使用本地的数据，因此会比较快，默认值是4。</p>
</li>
</ul>
<p><strong>shard rebalance settings</strong></p>
<p>比如集群中一共有5台机器，100个shard，负载均衡的情况下，每个机器上有20个shard。此时加入了一台新机器，就要触发rebalance操作，重新让整个集群负载均衡。这个过程就是shard rebalance。</p>
<ul>
<li><p>cluster.routing.rebalance.enable</p>
<ul>
<li>all，默认，允许对所有类型的shard进行rebalance过程</li>
<li>primaries，仅仅允许对primary shard进行rebalance过程</li>
<li>replicas，仅仅允许对replica shard进行rebalance</li>
<li>none，不允许对任何shard进行rebalance</li>
</ul>
</li>
<li><p>cluster.routing.allocation.allow_rebalance</p>
<ul>
<li>always，任何时候都允许rebalance</li>
<li>indices_primaries_active，仅仅只有在所有的primary shard都被分配之后才允许rebalance</li>
<li>indices_all_active，默认，仅仅允许所有的primary shard和replica shard都被分配之后，才能rebalance</li>
</ul>
</li>
<li><p>cluster.routing.allocation.cluster_concurrent_rebalance</p>
<p>允许控制多少个shard rebalance的操作同时运行，默认是2</p>
</li>
</ul>
<p><strong>shard balance heuristics</strong></p>
<ul>
<li>cluster.routing.allocation.balance.shard：设置每个node的shard分配的权重因子，默认是0.45f，提高权重因子，就会尽可能让均匀的shard分配给集群中的所有node</li>
<li>cluster.routing.allocation.balance.index：定义每个index在一个node上的shard数量因子，默认是0.55f，提高这个参数，就会尽可能让每个index的shard均匀分配到所有的node上</li>
<li>cluster.routing.allocation.balance.threshold：默认是1.0f，提高这个权重因子会导致集群对shard balance有更小的侵略性</li>
</ul>
<h5 id="disk-based-shard-allocation"><a href="#disk-based-shard-allocation" class="headerlink" title="disk-based shard allocation"></a>disk-based shard allocation</h5><p>es在进行shard allocation的时候，会充分考虑每一个node的可用磁盘空间</p>
<ul>
<li><p>cluster.routing.allocation.disk.threshold_enabled：默认是true，如果是false会禁用基于disk的考虑</p>
</li>
<li><p>cluster.routing.allocation.disk.watermark.low：控制磁盘使用率的低水位，默认是85%，如果一个节点的磁盘空间使用率已经超过了85%，那么就不会分配shard给这个node了</p>
</li>
<li><p>cluster.routing.allocation.disk.watermark.high：控制磁盘使用率的高水位，默认是90%，如果一个节点的磁盘空间使用率已经超过90%了，那么就会将这个node上的部分shard移动走</p>
</li>
<li><p>cluster.info.update.interval：es检查集群中每个node的磁盘使用率的时间间隔，默认是30s</p>
</li>
<li><p>cluster.routing.allocation.disk.include_relocations：默认是true，意味着es在计算一个node的磁盘使用率的时候，会考虑正在分配给这个node的shard。</p>
</li>
</ul>
<h5 id="shard-allocation-awareness"><a href="#shard-allocation-awareness" class="headerlink" title="shard allocation awareness"></a>shard allocation awareness</h5><p>机架感知特性</p>
<p>如果在一个物理机上运行多个虚拟机，并且在多个虚拟机中运行了多个es节点，或者在多个机架上，多个机房，都有可能有多个es节点在相同的物理机上，或者在相同的机架上，或者在相同的机房里，那么这些节点就可能会因为物理机，机架，机房的问题，一起崩溃。</p>
<p>如果es可以感知到硬件的物理布局，就可以确保说，priamry shard和replica shard一定是分配到不同的物理机，或者物理机架，或者不同的机房，这样可以最小化物理机，机架，机房崩溃的风险。</p>
<p>shard allocation awareness可以设置node所在的机架，如果我们有多个机架，那么我们启动一个node的时候，就要告诉这个node它在哪个机架上，可以给它一个rack_id，比如下面的命令：./bin/elasticsearch -Enode.attr.rack_id=rack_one，也可以在elasticsearch.yml中设置这个机架id。</p>
<p>cluster.routing.allocation.awareness.attributes: rack_id<br>node.attr.rack_id=rack_one</p>
<p>上面的两行设置里，第一行是设置机架id的属性名称，第二行是用那个机架id属性名称设置具体的机架id。</p>
<p>如果启动两个node，都在一个机架上，此时创建一个有5个primary shard和5个replica shard的索引，此时shards会被分配到两个节点上</p>
<p>如果再启动两个node，设置为另外一个机架，此时es会将shard移动到新的node上，去确保说，不会让primary shard和其replica shard在同一个机架上。但是如果机架2故障了，为了恢复集群，那么还是会在恢复的时候，将shards全部在机架1上分配的。</p>
<p>prefer local shard机制：在执行search或者get请求的时候，如果启用了shard awareness特性，那么es会尽量使用local shard来执行请求，也就是在同一个awareness group中的shard来执行请求，也就是说尽量用一个机架或者一个机房中的shard来执行请求，而不要跨机架或者跨机房来执行请求</p>
<p>可以指定多个awareness属性，比如机架id和机房名称，类似下面：cluster.routing.allocation.awareness.attributes: rack_id,zone</p>
<p>强制性的感知</p>
<p>如果现在我们有两个机房，并且有足够的硬件资源来容纳所有的shard，但是可能每个机房的硬件只能容纳一半shard，不能容纳所有的shard。如果仅仅使用原始的感知特性，如果一个机房故障了，那么es会将需要恢复的shard全部分配给剩下的一个机房，但是剩下的那个机房的硬件资源并不足以容纳所有的shard。</p>
<p>强制感知特性会解决这个问题，因为这个特性会绝对不允许在一个机房内分配所有的shard</p>
<p>比如说，有一个感知属性叫做zone，有两个机房，zone1和zone2，看看下面的配置：</p>
<p>cluster.routing.allocation.awareness.attributes: zone<br>cluster.routing.allocation.awareness.force.zone.values: zone1,zone2 </p>
<p>那么此时如果将2个node分配给zone1机房，然后创建一个索引，5个primary shard和5个replica shard，但是此时只会在zone1机房分配5个primary shard，只有我们启动一批node在zone2机房，才会分配replica shard</p>
<h5 id="shard-allocation-filtering"><a href="#shard-allocation-filtering" class="headerlink" title="shard allocation filtering"></a>shard allocation filtering</h5><p>shard allocation filtering可以让我们允许或者不允许某些index的shard分配给一些特殊的节点，如果我们要下线一些node，就可以用这个feature禁止shard分配给这些即将下线的node，而且我们还可以将这些即将下线的节点的shard移动到其他节点。</p>
<p>用下面的命令可以下线一个节点，因为就不允许将shard分配给这个节点了</p>
<figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ada">PUT _cluster/settings<br>&#123;<br>  <span class="hljs-string">&quot;transient&quot;</span> : &#123;<br>    <span class="hljs-string">&quot;cluster.routing.allocation.exclude._ip&quot;</span> : &quot;10.0.0.1&quot;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h5 id="node下线时的shard延迟分配"><a href="#node下线时的shard延迟分配" class="headerlink" title="node下线时的shard延迟分配"></a>node下线时的shard延迟分配</h5><p>如果从集群中下线一个节点，master会做下面这些事情：</p>
<ol>
<li>如果那个节点上有primary shard，那么master会将那些primary shard在其他节点上的replica shard提升为primary shard</li>
<li>分配新的replica shard来保证replica数量充足</li>
<li>在剩下的各个node上进行shard rebalance，确保负载均衡</li>
</ol>
<p>这些操作可以保护集群不会丢失数据，因为会对每个shard都复制充足的replica shard</p>
<p>但是这个过程，可能会导致集群中出现很重的负载，包括网络负载和磁盘IO负载，如果那个下线的节点只是因为故障被下线，马上就会有新的节点来顶替它，那么这种立即执行的shard recovery过程是不需要的，考虑下面的场景：</p>
<ol>
<li>某个node跟集群丢失了网络连接</li>
<li>master node将那个node上的primary shard对应的其他节点上的replica shard提升为primary shard</li>
<li>master node分配新的replica shard到其他节点上</li>
<li>每个新的replica shard都会通过网络传输一份primary shard的完整的副本数据</li>
<li>很多shard都被移动到其他的node来让集群rebalance</li>
<li>但是几分钟以后，那个丢失了网络连接的node又重新连接到了集群中</li>
<li>master节点又要再次进行rebalance操作，因为需要将一些shard分配给那个node</li>
</ol>
<p>其实如果master node也许只要等待几分钟，那么丢失的那个node自己会回来，丢失的shard也会自动恢复过来，因为数据都在节点的本地，不需要重新拷贝数据以及网络传输，这个过程是非常快速的</p>
<p>index.unassigned.node_left.delayed_timeout，这个参数可以设置某个节点下线之后，对应的replica shard被重新复制和分配的时间等待期，默认是1m，可以通过下面的命令来修改：</p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sqf">PUT <span class="hljs-variable">_all</span>/<span class="hljs-variable">_settings</span><br>&#123;<br>  <span class="hljs-string">&quot;settings&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;index.unassigned.node_left.delayed_timeout&quot;</span>: <span class="hljs-string">&quot;5m&quot;</span><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>如果启用了delayed allocation之后，那么就会看到下面的场景：</p>
<ol>
<li>某个node丢失了网络连接</li>
<li>master将那个node上的一些primary shard对应的其他node上的replica shard提升为primary shard</li>
<li>master记录下来一条消息日志，这个primary shard的replica shard还没有重新分配和开始，被delayed了，会等待1m</li>
<li>cluster会保持yellow状态，因为没有足够的replica shard</li>
<li>那个丢失了的node在几分钟之后，如果回到了集群中</li>
<li>缺失的那些replica shard会直接分配给那个node，使用其本地的数据即可</li>
</ol>
<p>如果某个node确定了肯定不会再回到集群中，那么可以通过下面的命令，手动设置一下，直接不等待那个节点回来了</p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sqf">PUT <span class="hljs-variable">_all</span>/<span class="hljs-variable">_settings</span><br>&#123;<br>  <span class="hljs-string">&quot;settings&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;index.unassigned.node_left.delayed_timeout&quot;</span>: <span class="hljs-string">&quot;0&quot;</span><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h5 id="索引恢复的优先级"><a href="#索引恢复的优先级" class="headerlink" title="索引恢复的优先级"></a>索引恢复的优先级</h5><p>没有被分配的shard都是按照优先级来分配的，有下面几个优先级，index.priority，索引的创建日期，索引名称</p>
<figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs dts">PUT index_3<br>&#123;<br>  <span class="hljs-string">&quot;settings&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;index.priority&quot;</span>: <span class="hljs-number">10</span><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h5 id="每个节点的shard数量"><a href="#每个节点的shard数量" class="headerlink" title="每个节点的shard数量"></a>每个节点的shard数量</h5><p>cluster.routing.allocation.total_shards_per_node，设置每个节点最多承载的shard数量，默认是无限制的</p>
<h4 id="gateway"><a href="#gateway" class="headerlink" title="gateway"></a>gateway</h4><p>gateway是elasticsearch底层的一个module，负责用来存储每个es节点的cluster state的，里面包含了一些集群中的node信息，每个node的负载，资源，索引，每个索引的shard在各个node上分配的一些信息等，node重启的时候，gateway也会负责将本地磁盘上的cluster state给它读取出来，放入内存中。</p>
<p>以下的设置，必须在每个master候选节点上都进行设置</p>
<ul>
<li><p>gateway.expected_nodes：要求必须有多少个节点在集群中，当加入集群中的节点数量达到这个期望数值之后，每个node的local shard的恢复就会立即开始，默认的值是0，也就是不会做任何的等待</p>
</li>
<li><p>gateway.expected_master_nodes：要求必须有多少个master node在集群中，只要有这个数量的master node加入了集群，每个node的local shard recovery就会立即开始，默认的值是0</p>
</li>
<li><p>gateway.expected_data_nodes：要求必须有多少个data node在集群中，只要有这个数量的master node加入了集群，每个node的local shard recovery就会立即开始，默认的值是0</p>
</li>
<li><p>gateway.recover_after_time：如果期望的节点数量没有达标，那么会等待一定的时间，然后就开始进行shard recovery，默认是等待5m</p>
</li>
</ul>
<p>如果gateway.recover_after_time时间范围内，指定数量的节点还没有加入集群，但是只要满足下面的条件之一就会立即开始恢复</p>
<ul>
<li><p>gateway.recover_after_nodes：只要指定数量的节点加入集群，就开始进行恢复</p>
</li>
<li><p>gateway.recover_after_master_nodes：只要指定数量的master节点加入集群，就开始进行恢复</p>
</li>
<li><p>gateway.recover_after_data_nodes：只要指定数量的data node加入集群，就开始恢复</p>
</li>
</ul>
<p>比如说，集群中一共有10个node，如果10分钟内10个node全部上线，就会立即开始shard recovery，如果等待超过10分钟，并且node数量超过了8个也会进行shard recovery。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">gateway</span>.recover_after_nodes: <span class="hljs-number">8</span><br><span class="hljs-attribute">gateway</span>.expected_nodess: <span class="hljs-number">10</span><br><span class="hljs-attribute">gateway</span>.recover_after_time: <span class="hljs-number">10</span>m<br></code></pre></td></tr></table></figure>

<h4 id="http、network和transport"><a href="#http、network和transport" class="headerlink" title="http、network和transport"></a>http、network和transport</h4><p>HTTP module就是es的http api模块，主要是用来对外提供请求接口服务的，对外端口可以通过http.port设置，默认在9200~9300之间选择一个，优先选择9200，如果被绑定，则选择9201，以此类推。</p>
<p>es的http机制是完全异步的，也就是说线程不会因为等待响应而陷入阻塞，http异步通信机制的优点就是解决了C10k问题。如果可能的话，尽量使用http keep alive，可以提升性能，而且可以避免客户端发生http chunking现象。</p>
<p>network module</p>
<p>es默认是绑定到localhost的，这只能让es运行在开发模式下，可以通过network.host设置，绑定的是本地的回环地址：127.0.0.1，进入的是development mode 开发模式。如果将network.host设置为比如192.168.31.187之类的这种hostname或者ip地址之后，进入production mode，生产模式</p>
<p>transport module</p>
<p>transport是用来进行节点间的互相通信的模块</p>
<p>通过transport.tcp.port配置，用于配置节点间互相通信的端口号，默认是9300，范围在9300~9400之间，优先绑定9300，如果被占用则用9301，以此类推。</p>
<p>transport module，es各个node之间，其实也会进行频繁的通信，比如交换cluster state，reqeust transfer，比如插入一条document，路由之后，应该是到node3的shard2上去处理，但是请求可能发送到的是node1的shard0上，node1就要将这个document index的请求转发给node3，让node3上的shard2去处理这个请求。</p>
<h4 id="thread-pool"><a href="#thread-pool" class="headerlink" title="thread pool"></a>thread pool</h4><p>每个节点都有多个thread pool，这样可以提升多线程处理的能力，这些线程池大多数都有一个对应的queue与其绑定，可以允许线程池满的时候，让pending的请求在队列里排队，而不是将pending请求抛弃掉。</p>
<ul>
<li><p>generic thread pool：应付一些普通的操作，比如后台的node discovery，thread pool类型是scaling</p>
</li>
<li><p>index thread pool：用于进行index和delete操作，是fixed类型，大小为cpu core数量，queue大小是200，这个线程池的最大大小是cpu core数量 + 1</p>
</li>
<li><p>search thread pool：用于search操作，是fixed类型，大小是cpu core数量 * 3 / 2 + 1，queue大小是1000</p>
</li>
<li><p>get thread pool：用于get操作，是fixed类型，大小是cpu core数量，队列大小是1000</p>
</li>
<li><p>bulk thread pool：用于bulk操作，是fixed类型，大小是cpu core数量，queue大小是200，最大的线程池大小是cpu core数量 + 1</p>
</li>
<li><p>snapshot thread pool：用于snapshot/restore操作，是scaling类型，每个线程存活时间为5m，最大数量是min(5, cpu core数量 / 2)</p>
</li>
<li><p>refresh thread pool：用于refresh操作，是scaling类型，存活时间为5m，最大数量是min(10, cpu core数量 / 2)</p>
</li>
</ul>
<p>在elasticsearch.yml配置文件中，按照下面的格式对thread pool进行配置</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">thread_pool:</span><br>    <span class="hljs-attr">bulk:</span><br>        <span class="hljs-attr">size:</span> <span class="hljs-number">16</span><br>        <span class="hljs-attr">queue_size:</span> <span class="hljs-number">1000</span><br></code></pre></td></tr></table></figure>

<p>线程池类型：</p>
<ul>
<li><p>fixed类型线程池：线程数量是固定的，同时绑定一个queue用于存放pending request</p>
</li>
<li><p>scaling类型：这种线程池数量是可变的，根据负载来变化，最小是cpu core数量，最大是其公式定义，keep_alive参数可以控制其线程空闲多长时间被释放</p>
</li>
</ul>
<p>thread_pool:<br>    refresh:<br>        core: 1<br>        max: 8<br>        keep_alive: 2m</p>
<p>在elasticsearch.yml配置文件中可以对cpu core数量进行设置</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">processors</span>: <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure>

<p>通过上面的参数可以显示设置cpu core数量，意义有下面3点：</p>
<ol>
<li><p>如果在一台机器上运行了多个es节点，但是可能只想要让每个es节点使用部分cpu core，而不是物理机上的所有cpu core，就可以手动设置。比如一台物理机，上面的cpu core是16个，运行了两个es节点，此时就可以手动设置processors是8，就是让每个es节点仅仅使用8个cpu core</p>
</li>
<li><p>默认cpu core的数量最大限制为32个，所以如果我们如果物理机超过了32个cpu core，那么可以手动设置。比如说你的物理机的cpu core是64个，但是此时es会去使用的cpu core可能也就32个，最大限制，此时就是要手动设置processors是64。</p>
</li>
<li><p>有时候可能会捕获到错误的cpu core数量，此时需要手动设置</p>
</li>
</ol>
<h4 id="plugin"><a href="#plugin" class="headerlink" title="plugin"></a>plugin</h4><p>plugin module，主要就是用来负责插件的管理，安装插件，查看插件，删除插件，更新插件</p>
<ul>
<li><p>安装plugin：elasticsearch-plugin install [plugin_name]</p>
</li>
<li><p>通过url安装plugin：elasticsearch-plugin install [url]</p>
</li>
<li><p>通过本地包离线安装plugin：elasticsearch-plugin install file:///path/to/plugin.zip</p>
</li>
<li><p>查看plugin list：elasticsearch-plugin list</p>
</li>
<li><p>删除plugin：elasticsearch-plugin remove [pluginname]，如果删除了一个plugin，必须重启节点才能生效</p>
</li>
<li><p>升级/更新plugin：先删除，再安装，然后重启节点</p>
</li>
</ul>
<h3 id="node"><a href="#node" class="headerlink" title="node"></a>node</h3><p>如果我们启动es的一个实例，那么就是启动了一个es node，一些es node就可以组成一个es集群。如果仅仅运行了一个es node，那么也有一个es集群，只是节点数量就是1。</p>
<p>集群中的每个node都可以处理http和transport请求，其中transport层是用来处理节点间的通信的，http层是用来处理外部的客户端rest请求的。</p>
<p>所有的node都知道集群中的其他node，并且可以将客户端的请求转发到适当的节点上去。</p>
<p>节点的类型包含以下几种：</p>
<ul>
<li><p>master-eligible node：master候选节点，将node.master设置为true（默认），代表这个node就是master的候选节点，可以被选举为master node，然后控制整个集群。</p>
</li>
<li><p>data node：将node.data设置为true（默认），data node可以存储数据，同时处理这些数据相关的操作，比如CRUD操作，搜索操作，聚合操作，等等。</p>
</li>
<li><p>ingest node：将node.ingest设置为true（默认），ingest node是用来对document写入索引文件之前进行预处理的。可以对每个document都执行一条ingest pipeline，在document写入索引文件之前，先对其数据进行处理和转化。但是如果要执行的ingest操作太过繁重，那么可以规划单独的一批ingest node出来，然后将node.master和node.data都设置为false即可。</p>
</li>
<li><p>tribe node：tribe node可以通过tribe.*相关参数来设置，它是一种特殊的coordinate node，可以连接到多个es集群上去，然后对多个集群执行搜索等操作。</p>
</li>
<li><p>默认情况下，每个node的node.master，node.data，node.ingest都是true，都是master候选节点，也可以作为data node存储和操作数据，同时也可以作为ingest node对数据进行预处理。对于小于20个节点的小集群来说，这种架构是ok的，没问题的。但是如果对于大于20个物理机的集群来说，最好是单独规划出master node、data node和ingest node来。</p>
</li>
<li><p>coordinate node：搜索和bulk等请求可能会涉及到多个节点上的不同shard里的数据，比如一个search请求，就需要两个阶段执行，首先第一个阶段就是一个coordinating node接收到这个客户端的search request。接着，coordinating node会将这个请求转发给存储相关数据的node，每个data node都会在自己本地执行这个请求操作，同时返回结果给coordinating node，接着coordinating node会将返回过来的所有的请求结果进行缩减和合并，合并为一个global结果。</p>
<p>每个node都是一个coordinating node。这就意味着如果一个node，将node.master，node.data，node.ingest全部设置为false，那么它就是一个纯粹的coordinating node，仅仅用于接收客户端的请求，同时进行请求的转发和合并。</p>
</li>
</ul>
<p>如果真的是大集群的话，最好也是单独规划一批node出来，就作为coordinating node，然后让es client全部往这些node上去发送请求。</p>
<p>如果真的是一个大于20个节点的生产集群的话，建议将4种node，master node，data node，ingest node，cooridating node，全部分离开来</p>
<p>如果集群中有30台机器：</p>
<ul>
<li>master node：3个</li>
<li>ingest node：视具体情况而定，具体是看你的ingest预处理操作有多么的复杂，耗费多少资源，但是一般情况下来说，es ingest node用的比较少的，ingest node也可以不用单独规划一批出来</li>
<li>coordinate node：视具体情况而定，但是对于大集群来说，最好是单独拆几个节点出来，用于接收客户端的请求，3个节点。主要是看你的并发访问量有多大，比如说你的最大的QPS也就是10，或者是100，那么3个节点肯定够了。如果你的QPS是1000，或者是10000，那么可能就要规划，10个coordinate node，或者100个</li>
<li>data node：24个data node，data node肯定是分配的是最多的，主要用来存储数据，执行各种对数据的操作么，资源耗费肯定是最多的</li>
</ul>
<p>master eligible node</p>
<p>master-eligible node的介绍以及配置</p>
<p>master node负责轻量级的集群管理工作，比如创建和删除索引，追踪集群中的每个node，决定如何将shards分配给各个node。对于集群来说，有一个稳定的master node，是非常关键的。然后master-eligible node都有机会被选举为一个master node，同时master node必须有权限访问path.data指定的data目录，因为master node需要在data目录中存储cluster state。</p>
<p>对数据进行index和search操作，会耗费大量的cpu，内存，磁盘io，以及网络io，耗费的是每个node的资源。因此我们必须要确保master node是非常稳定的，而且是压力不大的，对于大集群来说，比较好的办法是划分出单独的master node和data node。如果不拆开的话，一个node又要是data node，要复杂存储数据，处理各种操作，同时又要负责管理集群，可能就会不稳定，出问题。</p>
<p>同时因为默认情况下，master node也能扮演coordinating node的角色，并且将search和index请求路由到对应的data node上去执行，最好是不要让master node来执行这些coordinate操作。因为msater node的稳定运行对于整个集群来说非常重要，比你利用master node资源来执行一些coordinate操作要重要的多。</p>
<p>如果要设置一个node为专门的master-eligible node，需要做如下的设置：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">node.master:</span> <span class="hljs-literal">true</span> <br><span class="hljs-attr">node.data:</span> <span class="hljs-literal">false</span> <br><span class="hljs-attr">node.ingest:</span> <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure>

<p>通过minimum_master_nodes来避免脑裂问题</p>
<p>要预防数据的丢失，我们就必须设置discovery.zen.minimum_master_nodes参数为一个合理的值，这样的话，每个master-eligible node才知道至少需要多少个master-eligible node才能组成一个集群。</p>
<p>比如说，我们现在有一个集群，其中包含两个master-eligible nodes。然后一个网络故障发生了，这两个节点之间丢失了联络。每个节点都认为当前只有一个master-eligible node，就是它们自己。此时如果discovery.zen.minimum_master_nodes参数的默认值是1，那么每个node就可以让自己组成一个集群，选举自己为master node即可。结果就会导致出现了两个es集群，这就是脑裂现象。即使网络故障解决了，但是这两个master node是不可能重新组成一个集群了。除非某个master eligible node重启，然后自动加入另外一个集群，但是此时写入这个节点的数据就会彻底丢失。</p>
<p>那么如果现在我们有3个master-eligible node，同时将discovery.zen.minimum_master_nodes设置为2.如果网络故障发生了，此时一个网络分区有1个node，另外一个网络分区有2个node，只有一个node的那个网络分区，没法检测到足够数量的master-eligible node，那么此时它就不能选举一个master node出来组成一个新集群。但是有两个node的那个网络分区，它们会发现这里有足够数量的master-eligible node，那么就选举出一个新的master，然后组成一个集群。当网络故障解除之后，那个落单的node就会重新加入集群中。</p>
<p>discovery.zen.minimum_master_nodes，必须设置为master-eligible nodes的quorum，quorum的公式为：(master_eligible_nodes / 2) + 1。</p>
<p>换句话来说，如果有3个master-eligible nodes，那么那个参数就必须设置为(3 / 2) + 1 = 2，比如下面这样：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">discovery</span><span class="hljs-selector-class">.zen</span><span class="hljs-selector-class">.minimum_master_nodes</span>: 2<br></code></pre></td></tr></table></figure>

<p>随着集群节点的上线和下限，这个参数都是要重新设置的，可以通过api来设置</p>
<figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs dts">PUT _cluster/settings<br>&#123;<br>  <span class="hljs-string">&quot;transient&quot;</span>: &#123;<br>    <span class="hljs-string">&quot;discovery.zen.minimum_master_nodes&quot;</span>: <span class="hljs-number">2</span><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>此时将master node和data node分离的好处就出来了，一般如果单独规划一个master nodes的话，只要规划固定的3个node是master-eligible node就可以了，那么data node无论上线和下限多少个，都无所谓的。</p>
<p>data node</p>
<p>data node负责存储shard的数据，也就是那些document。data node可以处理各种操作，比如CRUD，搜索，聚合。这些操作全都是很耗费IO，内存和cpu资源的。因此监控这些资源的使用是很重要的，同时如果资源过载了，那么就要添加更多的data node。</p>
<p>如果要设置一个专门的data node，需要做出如下的设置：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">node.master:</span> <span class="hljs-literal">false</span> <br><span class="hljs-attr">node.data:</span> <span class="hljs-literal">true</span> <br><span class="hljs-attr">node.ingest:</span> <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure>

<p>ingest node</p>
<p>nigest node可以执行预处理pipeline，包含了多个ingest processors。不同的ingest processor执行的操作类型是不同的，那么对资源的需求也是不同的，不过还是最好是规划一批单独的ingest node出来，不要跟master node和data node混合在一起。</p>
<p>如果要配置一个单独的ingest node：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">node.master:</span> <span class="hljs-literal">false</span> <br><span class="hljs-attr">node.data:</span> <span class="hljs-literal">false</span> <br><span class="hljs-attr">node.ingest:</span> <span class="hljs-literal">true</span> <br><span class="hljs-attr">search.remote.connect:</span> <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure>

<p>cooridnating only node</p>
<p>如果我们规划了一批专门的master node，data node以及ingest node，那么此时还遗留下来了一种node，那就是coordinating node，这些node专门用来接收客户端的请求，同时对请求进行路由和转发，并对请求的结果进行合并。</p>
<p>coordinating only nodes对于大集群来说，可以使用专门的node来负载coordinate操作，而不是让coordinate操作的工作负载集中到master node和data node上去。coordinating node也会加入cluster，同时可以获取到完整的cluster state，它们主要是用cluster state中包含的node info来进行请求转发。</p>
<p>如果在一个集群中规划太多的coordinating node可能会加重整个集群的负担，因为被选举出来的master node必须要从所有的node上得到cluster state update的ack，如果coordinating nodes过多，那么可能会加重master node的负担。</p>
<p>如果要设置coordinating only node的话：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">node.master:</span> <span class="hljs-literal">false</span> <br><span class="hljs-attr">node.data:</span> <span class="hljs-literal">false</span> <br><span class="hljs-attr">node.ingest:</span> <span class="hljs-literal">false</span> <br><span class="hljs-attr">search.remote.connect:</span> <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure>

<p>node data path设置</p>
<p>（1）path.data</p>
<p>每个data和master-eligible node都需要能够访问data目录，在那里存储了每个shard的数据，包括cluster state也存储在那里。path.data默认是指向$ES_HOME/data目录的，但是在生产环境中，肯定是不能这样设置的，因为在升级es的时候，可能会导致数据被清空或者覆盖。</p>
<p>此时一般需要在elasticsearch.yml中设置path.data：</p>
<p>path.data:  /var/elasticsearch/data</p>
<p>（2）node.max_local_storage_nodes</p>
<p>data目录可以被多个node共享，即使是不同集群中的es node，也许他们在一个物理机上启动了。这个共享的方式对于我们测试failover是很有用的，以及在开发机上测试不同的配置。但是在生产环境下，绝对不用这么做，一个data目录就给一个es node使用即可。默认情况下，es被配置成阻止超过一个node共享data目录中的数据，如果要允许多个node共享一个data目录，需要设置node.max_local_storage_nodes为一个超过1的数字。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/blog/categories/Elasticsearch/">Elasticsearch</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/blog/tags/elasticsearch/">elasticsearch</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/blog/2022/10/12/Elasticsearch/elasticsearch%E9%85%8D%E7%BD%AE/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Elasticsearch配置</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/blog/2022/10/12/Elasticsearch/elasticsearch%E5%9F%BA%E7%A1%80/">
                        <span class="hidden-mobile">Elasticsearch基础</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":200})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.staticfile.org/jquery/3.5.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.5.3/js/bootstrap.min.js" ></script>
<script  src="/blog/js/events.js" ></script>
<script  src="/blog/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/blog/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.staticfile.org/tocbot/4.12.0/tocbot.min.js" ></script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>






  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/blog/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/blog/local-search.xml";
      jQuery('#local-search-input').on('click', function() {
        searchFunc(path, 'local-search-input', 'local-search-result');
      });
      jQuery('#modalSearch').on('shown.bs.modal', function() {
        jQuery('#local-search-input').focus();
      });
    })()
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/blog/js/boot.js" ></script>


</body>
</html>
